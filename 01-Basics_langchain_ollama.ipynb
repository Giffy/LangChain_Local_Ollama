{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb92356d",
   "metadata": {},
   "source": [
    "To run this notebook is it required to have ollama running in port 11434 (default port).\n",
    "\n",
    "You can run it on windows, mac or linux, download from https://ollama.com/download \n",
    "\n",
    "Or on a docker container https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\n",
    "\n",
    "Two models are used:\n",
    "- qwen2.5:0.5b\n",
    "- granite4:1b\n",
    "\n",
    "Download them using the command:\n",
    "```\n",
    "ollama pull qwen2.5:0.5b\n",
    "ollama pull granite4:1b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13dbac1",
   "metadata": {},
   "source": [
    "### Test Ollama connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedf5fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "# https://docs.langchain.com/oss/python/integrations/chat/ollama\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "MODEL=\"qwen2.5:0.5b\"    # ollama pull qwen2.5:0.5b\n",
    "\n",
    "client = ChatOllama(model=MODEL,\n",
    "                    temperature=0\n",
    "                    )\n",
    "\n",
    "messages = [(\"system\", \"Just reply with two words\"), \n",
    "            (\"human\", \"Hi\"),\n",
    "            ]\n",
    "\n",
    "response = client.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bdff2",
   "metadata": {},
   "source": [
    "### Tutorial LangChain but updated to use Ollama from https://docs.langchain.com/oss/python/langchain/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b03bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parrots can mimic sounds and expressions due to their evolutionary adaptation to live in human societies.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "load_dotenv()  # Loads .env variables as environment variables\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING\"] = \"false\"\n",
    "model = init_chat_model(model=MODEL, model_provider=\"ollama\")\n",
    "response = model.invoke(\"Why do parrots talk? in 5 words\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12304636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par|rots|'| vibrant| plum|age| is| due| to| pig|ments| and| ultr|aviolet| light| absorption|.|||"
     ]
    }
   ],
   "source": [
    "# Basic streaming response\n",
    "for chunk in model.stream(\"Why do parrots have colorful feathers? in 5 words\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744b59e",
   "metadata": {},
   "source": [
    "### Build a basic agent Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e565b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='9346f88f-8f50-48b6-b33f-dfd0c29e216c'),\n",
       "  AIMessage(content=\"To provide you with the current weather forecast for San Francisco (SF), I'll need to know your location. Could you please tell me the name of the city where you are? If you don't have a specific name, just give me the short address or city abbreviation.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:0.5b', 'created_at': '2025-12-14T21:24:32.0375776Z', 'done': True, 'done_reason': 'stop', 'total_duration': 922491600, 'load_duration': 180212300, 'prompt_eval_count': 146, 'prompt_eval_duration': 19141400, 'eval_count': 56, 'eval_duration': 634106400, 'logprobs': None, 'model_name': 'qwen2.5:0.5b', 'model_provider': 'ollama'}, id='lc_run--019b1ec0-330a-7cb2-97ce-b8a89100af20-0', usage_metadata={'input_tokens': 146, 'output_tokens': 56, 'total_tokens': 202})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=init_chat_model(model=MODEL, model_provider=\"ollama\"),\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a363a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'validate_user', 'args': {'addresses': ['123 Fake St', '234 Pretend Boulevard'], 'user_id': 123}, 'id': '2d5eb458-8152-45a6-a443-176a5047963c', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.messages import AIMessage\n",
    "from langchain.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "@tool\n",
    "def validate_user(user_id: int, addresses: List[str]) -> bool:\n",
    "    \"\"\"Validate user using historical addresses.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): the user ID.\n",
    "        addresses (List[str]): Previous addresses as a list of strings.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=MODEL,\n",
    "                 validate_model_on_init=True,\n",
    "                 temperature=0,\n",
    "                ).bind_tools([validate_user])\n",
    "\n",
    "result = llm.invoke(\n",
    "    \"Could you validate user 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX.\"\n",
    ")\n",
    "\n",
    "if isinstance(result, AIMessage) and result.tool_calls:\n",
    "    print(result.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e85127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expression \"3^3\" represents the mathematical operation of exponentiation.\n",
      "\n",
      "In math, exponentiation means raising a number to another power, which is denoted by an \"exponentiation sign\" (x) and followed by the base number with a caret (^), which shifts the original base into the new position or power. For example:\n",
      "\n",
      "- 2^3 = 2 * 2 * 2\n",
      "- 5^2 = 5 * 5\n",
      "\n",
      "In this case, 3 is raised to the power of itself, which is just 3:\n",
      "\n",
      "3^3 = 3 * 3 * 3 = 27\n",
      "\n",
      "So, \"3^3\" equals 27.\n"
     ]
    }
   ],
   "source": [
    "# Using a non reasoning model\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=MODEL)\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\"What is 3^3?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce31483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expression 3^3 refers to the mathematical operation of exponentiation, which means multiplying the base (in this case, 3) by itself for a specified number of times equal to the exponent (also in this case, 3). \n",
      "\n",
      "So, 3^3 equals:\n",
      "\n",
      "3 * 3 * 3 = 27\n",
      "\n",
      "Therefore, the answer is 27.\n"
     ]
    }
   ],
   "source": [
    "# Using a reasoning model\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain_core.messages import ChatMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"granite4:1b\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"control\", content=\"thinking\"),\n",
    "    HumanMessage(\"What is 3^3?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91de8cc",
   "metadata": {},
   "source": [
    "### Build a real-world agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3ecdd",
   "metadata": {},
   "source": [
    "This example is available in https://docs.langchain.com/oss/python/langchain/quickstart using Claude Sonnet 4.5.\n",
    "For this example I used Ollama an local model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ba4bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='granite4:1b'       # ollama pull granite4:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c39ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "            You have access to two tools:\n",
    "            - get_weather_for_location: use this to get the weather for a specific location\n",
    "            - get_user_location: use this to get the user's location\n",
    "            If a user asks you for the weather, make sure you know the location. If you can tell \n",
    "            from the question that they mean wherever they are, use the get_user_location tool \n",
    "            to find their location.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tools\n",
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    print(f'get_weather_argument : {city}')\n",
    "    if city == \"Florida\":\n",
    "        return f\"It's always sunny in {city}!\"\n",
    "    return f\"It's raining in {city}!\"\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    print(f'get_user_location_argument : {user_id}')\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "640628e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model=init_chat_model(model=MODEL, \n",
    "                      model_provider=\"ollama\",\n",
    "                      temperature=0.5,\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4136c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1330ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def responseFormat(response):\n",
    "    for item in response[\"messages\"]:\n",
    "        if isinstance(item, HumanMessage):\n",
    "            if item.content:\n",
    "                print(f\"Query : {item.content}\")\n",
    "        if isinstance(item, AIMessage):\n",
    "            if item.content:\n",
    "                print(f\"AI response: {item.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd678752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run model\n",
    "from langchain.messages import AIMessage\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "17004472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_user_location_argunment : 1\n",
      "get_weather_argunment : Florida\n",
      "Query : Tell me my location and what the weather outside is.\n",
      "AI response: The weather outside is currently sunny and warm, perfect for a day of sunshine!\n",
      "Query : thank you!\n",
      "AI response: You're welcome! If you need anything else, just let me know.\n"
     ]
    }
   ],
   "source": [
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me my location and what the weather outside is.\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "# # Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "responseFormat(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "58f891a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_user_location_argunment : 2\n",
      "get_weather_argunment : <user's location>\n",
      "Query : Tell me my location and what the weather outside is.\n",
      "AI response: The weather is rainy in SF! Looks like it's time to grab an umbrella.\n",
      "Query : thank you!\n",
      "AI response: You're welcome! If you need anything else, just let me know.\n"
     ]
    }
   ],
   "source": [
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me my location and what the weather outside is.\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"2\")\n",
    ")\n",
    "\n",
    "# # Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"2\")\n",
    ")\n",
    "\n",
    "responseFormat(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0567d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-local-ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
