{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb92356d",
   "metadata": {},
   "source": [
    "To run this notebook is it required to have ollama running in port 11434 (default port).\n",
    "\n",
    "You can run it on windows, mac or linux, download from https://ollama.com/download \n",
    "\n",
    "Or on a docker container https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image\n",
    "\n",
    "Two models are used:\n",
    "- qwen2.5:0.5b\n",
    "- granite4:1b\n",
    "\n",
    "Download them using the command:\n",
    "```\n",
    "ollama pull qwen2.5:0.5b\n",
    "ollama pull granite4:1b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13dbac1",
   "metadata": {},
   "source": [
    "### Test Ollama connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dedf5fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "# https://docs.langchain.com/oss/python/integrations/chat/ollama\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "MODEL=\"qwen2.5:0.5b\"    # ollama pull qwen2.5:0.5b\n",
    "\n",
    "client = ChatOllama(model=MODEL,\n",
    "                    temperature=0, # Adjust temperature for creativity (0.0 to 1.0)\n",
    "                    api_base=\"http://localhost:11434\"\n",
    "                    )\n",
    "\n",
    "messages = [(\"system\", \"Just reply with two words\"), \n",
    "            (\"human\", \"Hi\"),\n",
    "            ]\n",
    "\n",
    "response = client.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1dc83c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'qwen2.5:0.5b',\n",
       " 'created_at': '2025-12-14T23:14:11.7763627Z',\n",
       " 'done': True,\n",
       " 'done_reason': 'stop',\n",
       " 'total_duration': 2545068600,\n",
       " 'load_duration': 2495714800,\n",
       " 'prompt_eval_count': 19,\n",
       " 'prompt_eval_duration': 25763000,\n",
       " 'eval_count': 3,\n",
       " 'eval_duration': 16261900,\n",
       " 'logprobs': None,\n",
       " 'model_name': 'qwen2.5:0.5b',\n",
       " 'model_provider': 'ollama'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bdff2",
   "metadata": {},
   "source": [
    "### Tutorial LangChain but updated to use Ollama from https://docs.langchain.com/oss/python/langchain/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3b03bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They mimic human speech.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "load_dotenv()  # Loads .env variables as environment variables\n",
    "\n",
    "MODEL=\"qwen2.5:1.5b\"    # ollama pull qwen2.5:1.5b\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING\"] = \"false\"\n",
    "model = init_chat_model(model=MODEL, model_provider=\"ollama\")\n",
    "response = model.invoke(\"Why do parrots talk? in 5 words\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12304636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To| communicate| and| attract| mates|.|||"
     ]
    }
   ],
   "source": [
    "# Basic streaming response\n",
    "for chunk in model.stream(\"Why do parrots have colorful feathers? in 5 words\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "38e01ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's one for you: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything.\n"
     ]
    }
   ],
   "source": [
    "# types of Messages\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "messages_dict = [\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    "]\n",
    "# Or\n",
    "messages_dict = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "\t{\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "\t{\"role\": \"assistant\", \"content\": \"I am doing well, thank you! How can I assist you today?\"},\n",
    "\t{\"role\": \"user\", \"content\": \"Can you tell me a joke?\"},\n",
    "]\n",
    "# Or\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"Hello, how are you?\"),\n",
    "    AIMessage(\"I am doing well, thank you! How can I assist you today?\"),\n",
    "    HumanMessage(\"Can you tell me a joke?\"),\n",
    "]\n",
    "\n",
    "llm = init_chat_model(model=MODEL, model_provider=\"ollama\")\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744b59e",
   "metadata": {},
   "source": [
    "### Build a basic agent Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9e565b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='a36d0e72-7dc9-4688-b729-52484a78e677'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen2.5:1.5b', 'created_at': '2025-12-14T23:46:50.1109029Z', 'done': True, 'done_reason': 'stop', 'total_duration': 404087300, 'load_duration': 103656700, 'prompt_eval_count': 146, 'prompt_eval_duration': 63390800, 'eval_count': 20, 'eval_duration': 205828200, 'logprobs': None, 'model_name': 'qwen2.5:1.5b', 'model_provider': 'ollama'}, id='lc_run--019b1f42-7ce9-79c2-a4b8-b432577202ae-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': '2ad104cf-2aa7-459b-8569-4ff9bef66525', 'type': 'tool_call'}], usage_metadata={'input_tokens': 146, 'output_tokens': 20, 'total_tokens': 166}),\n",
       "  ToolMessage(content=\"It's always sunny in sf!\", name='get_weather', id='d64da470-2c99-475b-b72e-9c4c9bb568bf', tool_call_id='2ad104cf-2aa7-459b-8569-4ff9bef66525'),\n",
       "  AIMessage(content='The weather is currently clear and bright all day long in San Francisco! Enjoy the sunshine.', additional_kwargs={}, response_metadata={'model': 'qwen2.5:1.5b', 'created_at': '2025-12-14T23:46:50.4372337Z', 'done': True, 'done_reason': 'stop', 'total_duration': 320072900, 'load_duration': 109213100, 'prompt_eval_count': 189, 'prompt_eval_duration': 17579900, 'eval_count': 19, 'eval_duration': 165099100, 'logprobs': None, 'model_name': 'qwen2.5:1.5b', 'model_provider': 'ollama'}, id='lc_run--019b1f42-7e85-7501-8c40-40f0780bb911-0', usage_metadata={'input_tokens': 189, 'output_tokens': 19, 'total_tokens': 208})]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=init_chat_model(model=MODEL, model_provider=\"ollama\"),\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a363a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'validate_user', 'args': {'addresses': ['123 Fake St', '234 Pretend Boulevard'], 'user_id': 123}, 'id': '2d5eb458-8152-45a6-a443-176a5047963c', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.messages import AIMessage\n",
    "from langchain.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "@tool\n",
    "def validate_user(user_id: int, addresses: List[str]) -> bool:\n",
    "    \"\"\"Validate user using historical addresses.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): the user ID.\n",
    "        addresses (List[str]): Previous addresses as a list of strings.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=MODEL,\n",
    "                 validate_model_on_init=True,\n",
    "                 temperature=0,\n",
    "                ).bind_tools([validate_user])\n",
    "\n",
    "result = llm.invoke(\n",
    "    \"Could you validate user 123? They previously lived at \"\n",
    "    \"123 Fake St in Boston MA and 234 Pretend Boulevard in \"\n",
    "    \"Houston TX.\"\n",
    ")\n",
    "\n",
    "if isinstance(result, AIMessage) and result.tool_calls:\n",
    "    print(result.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e85127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expression \"3^3\" represents the mathematical operation of exponentiation.\n",
      "\n",
      "In math, exponentiation means raising a number to another power, which is denoted by an \"exponentiation sign\" (x) and followed by the base number with a caret (^), which shifts the original base into the new position or power. For example:\n",
      "\n",
      "- 2^3 = 2 * 2 * 2\n",
      "- 5^2 = 5 * 5\n",
      "\n",
      "In this case, 3 is raised to the power of itself, which is just 3:\n",
      "\n",
      "3^3 = 3 * 3 * 3 = 27\n",
      "\n",
      "So, \"3^3\" equals 27.\n"
     ]
    }
   ],
   "source": [
    "# Using a non reasoning model\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=MODEL)\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\"What is 3^3?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce31483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expression 3^3 refers to the mathematical operation of exponentiation, which means multiplying the base (in this case, 3) by itself for a specified number of times equal to the exponent (also in this case, 3). \n",
      "\n",
      "So, 3^3 equals:\n",
      "\n",
      "3 * 3 * 3 = 27\n",
      "\n",
      "Therefore, the answer is 27.\n"
     ]
    }
   ],
   "source": [
    "# Using a reasoning model\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain_core.messages import ChatMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"granite4:1b\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"control\", content=\"thinking\"),\n",
    "    HumanMessage(\"What is 3^3?\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91de8cc",
   "metadata": {},
   "source": [
    "### Build a real-world agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3ecdd",
   "metadata": {},
   "source": [
    "This example is available in https://docs.langchain.com/oss/python/langchain/quickstart using Claude Sonnet 4.5.\n",
    "For this example I used Ollama an local model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ba4bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='granite4:1b'       # ollama pull granite4:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c39ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "            You have access to two tools:\n",
    "            - get_weather_for_location: use this to get the weather for a specific location\n",
    "            - get_user_location: use this to get the user's location\n",
    "            If a user asks you for the weather, make sure you know the location. If you can tell \n",
    "            from the question that they mean wherever they are, use the get_user_location tool \n",
    "            to find their location.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tools\n",
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    print(f'get_weather_argument : {city}')\n",
    "    if city == \"Florida\":\n",
    "        return f\"It's always sunny in {city}!\"\n",
    "    return f\"It's raining in {city}!\"\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    print(f'get_user_location_argument : {user_id}')\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "640628e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model=init_chat_model(model=MODEL, \n",
    "                      model_provider=\"ollama\",\n",
    "                      temperature=0.5,\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4136c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1330ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def responseFormat(response):\n",
    "    for item in response[\"messages\"]:\n",
    "        if isinstance(item, HumanMessage):\n",
    "            if item.content:\n",
    "                print(f\"Query : {item.content}\")\n",
    "        if isinstance(item, AIMessage):\n",
    "            if item.content:\n",
    "                print(f\"AI response: {item.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bd678752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run model\n",
    "from langchain.messages import AIMessage\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "17004472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_user_location_argunment : 1\n",
      "get_weather_argunment : Florida\n",
      "Query : Tell me my location and what the weather outside is.\n",
      "AI response: The weather outside is currently sunny and warm, perfect for a day of sunshine!\n",
      "Query : thank you!\n",
      "AI response: You're welcome! If you need anything else, just let me know.\n"
     ]
    }
   ],
   "source": [
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me my location and what the weather outside is.\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "# # Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "responseFormat(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "58f891a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_user_location_argunment : 2\n",
      "get_weather_argunment : <user's location>\n",
      "Query : Tell me my location and what the weather outside is.\n",
      "AI response: The weather is rainy in SF! Looks like it's time to grab an umbrella.\n",
      "Query : thank you!\n",
      "AI response: You're welcome! If you need anything else, just let me know.\n"
     ]
    }
   ],
   "source": [
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me my location and what the weather outside is.\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"2\")\n",
    ")\n",
    "\n",
    "# # Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"2\")\n",
    ")\n",
    "\n",
    "responseFormat(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626f496",
   "metadata": {},
   "source": [
    "### Build a real-world agent\n",
    "Based in \"Build advanced Agentic LLM Apps with LangGraph and LangChain\" by Enric Domingo\n",
    "\n",
    "**Github:** https://github.com/enricd/langgraph_agents_pyday_bcn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8ce0567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dune, Interstellar, Blade Runner 2049\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# Basic hardcoded tool\n",
    "@tool\n",
    "def search_movies(genre: str) -> str:\n",
    "    \"\"\"Search for movies by genre. Currently supports 'sci-fi', 'comedy', and 'action'.\"\"\"\n",
    "    # In a real app, this would query a movie database\n",
    "    movies = {\n",
    "        \"sci-fi\": \"Dune, Interstellar, Blade Runner 2049\",\n",
    "        \"comedy\": \"The Grand Budapest Hotel, Superbad, Knives Out\",\n",
    "        \"action\": \"Mad Max: Fury Road, John Wick, Mission Impossible\"\n",
    "    }\n",
    "    \n",
    "    return movies.get(genre.lower(), \"No movies found for that genre\")\n",
    "\n",
    "\n",
    "# Using the tool\n",
    "result = search_movies.invoke({\"genre\": \"sci-fi\"})\n",
    "\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ca6a724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"temperature_celsius\": 12.9, \"weather_code\": 3}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# More realistic tool that calls an API\n",
    "@tool\n",
    "def get_weather(latitude: float, longitude: float) -> str:\n",
    "    \"\"\"Get current temperature in Celsius and weather code for given coordinates.\n",
    "\n",
    "    Args:\n",
    "        latitude: Latitude coordinate\n",
    "        longitude: Longitude coordinate\n",
    "\n",
    "    Returns:\n",
    "        JSON string with temperature_celsius and weather_code (do not include the code in your response, translate it to plain English)\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"current\": \"temperature_2m,weather_code\",\n",
    "        \"temperature_unit\": \"celsius\"\n",
    "    }\n",
    "\n",
    "    weather = requests.get(url, params=params).json()[\"current\"]\n",
    "    temperature = weather[\"temperature_2m\"]\n",
    "    weather_code = weather[\"weather_code\"]\n",
    "    result = {\n",
    "        \"temperature_celsius\": temperature,\n",
    "        \"weather_code\": weather_code\n",
    "    }\n",
    "\n",
    "    return json.dumps(result)\n",
    "\n",
    "# Test a tool directly with Barcelona coordinates\n",
    "print(get_weather.invoke({\"latitude\": 41.38, \"longitude\": 2.17}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "31edb406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [{'name': 'get_weather', 'args': {'latitude': 41.38, 'longitude': 2.17}, 'id': '285d392e-6121-4800-b36b-06272bfbb994', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "# This doesn't work with models running in Ollama\n",
    "\n",
    "# Bind tools to the model\n",
    "tools = [get_weather, search_movies]\n",
    "model_with_tools = llm.bind_tools(tools)\n",
    "message = \"What's the weather like in Barcelona? (Barcelona's coordinates are approximately 41.38° N latitude and 2.17° E longitude)\"\n",
    "\n",
    "# The model can now decide to call tools\n",
    "response = model_with_tools.invoke(message)\n",
    "\n",
    "# Check if the model wants to call a tool, in that case, we won't see the response of the tool yet.\n",
    "print(\"Tool calls:\", response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5189c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The current temperature in Barcelona is approximately **13 °C**, and the weather condition is **partly cloudy** according to the weather code provided (code 3).\n"
     ]
    }
   ],
   "source": [
    "# Let's execute the tool and continue the conversation\n",
    "\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "# Execute the tool call\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    # Call the actual tool\n",
    "    if tool_call[\"name\"] == \"get_weather\":\n",
    "        result = get_weather.invoke(tool_call[\"args\"])\n",
    "    elif tool_call[\"name\"] == \"search_movies\":\n",
    "        result = search_movies.invoke(tool_call[\"args\"])\n",
    "\n",
    "    # Create a ToolMessage with the result\n",
    "    tool_message = ToolMessage(\n",
    "        content=result,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )\n",
    "\n",
    "    # Continue the conversation with the tool result\n",
    "    final_response = model_with_tools.invoke([\n",
    "        HumanMessage(content=message),\n",
    "        response,\n",
    "        tool_message\n",
    "    ])\n",
    "\n",
    "    final_response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56ce8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-local-ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
